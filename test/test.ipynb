{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files_in_directory(directory_path):\n",
    "    # Iterate over all the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Construct the full file path\n",
    "        old_file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Check if it's a file (not a directory)\n",
    "        if os.path.isfile(old_file_path):\n",
    "            # Construct the new file name by adding \"aws_\" prefix\n",
    "            new_file_name = \"aws_\" + filename\n",
    "            new_file_path = os.path.join(directory_path, new_file_name)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "\n",
    "# Usage example\n",
    "directory_path = '/Users/yiminglin/Documents/Codebase/Dataset/pdf_reverse/baseline_results/gpt4-vision-cleaned_predicted_kv_pairs_no_structure'  # Replace with the path to your directory\n",
    "rename_files_in_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: llm_Active_Employment.csv -> llmns_Active_Employment.csv\n",
      "Renamed: llm_id_88.csv -> llmns_id_88.csv\n",
      "Renamed: llm_id_59_87.csv -> llmns_id_59_87.csv\n",
      "Renamed: llm_DecertifiedOfficersRev_9622 Emilie Munson.csv -> llmns_DecertifiedOfficersRev_9622 Emilie Munson.csv\n",
      "Renamed: llm_id_53_55_64_86.csv -> llmns_id_53_55_64_86.csv\n",
      "Renamed: llm_id_133.csv -> llmns_id_133.csv\n",
      "Renamed: llm_id_23_29.csv -> llmns_id_23_29.csv\n",
      "Renamed: llm_id_61_63_77_142.csv -> llmns_id_61_63_77_142.csv\n",
      "Renamed: llm_id_17.csv -> llmns_id_17.csv\n",
      "Renamed: llm_id_14_42.csv -> llmns_id_14_42.csv\n",
      "Renamed: llm_id_157.csv -> llmns_id_157.csv\n",
      "Renamed: llm_id_143.csv -> llmns_id_143.csv\n",
      "Renamed: llm_id_10.csv -> llmns_id_10.csv\n",
      "Renamed: llm_id_152.csv -> llmns_id_152.csv\n",
      "Renamed: llm_id_12.csv -> llmns_id_12.csv\n",
      "Renamed: llm_id_18_28_45_48_51_57_60_70_72_79_81_89_91_92_94_95_97_99_102_105_113_117_118_119_122_125_131_132_137_139_150_v2.csv -> llmns_id_18_28_45_48_51_57_60_70_72_79_81_89_91_92_94_95_97_99_102_105_113_117_118_119_122_125_131_132_137_139_150_v2.csv\n",
      "Renamed: llm_22-274.releasable.csv -> llmns_22-274.releasable.csv\n",
      "Renamed: llm_Archived_Certifications.csv -> llmns_Archived_Certifications.csv\n",
      "Renamed: llm_id_15_20_39_71_73_76_83_96_100_101_103_108_110_114.csv -> llmns_id_15_20_39_71_73_76_83_96_100_101_103_108_110_114.csv\n",
      "Renamed: llm_id_116_151.csv -> llmns_id_116_151.csv\n",
      "Renamed: llm_id_18_28_45_48_51_57_60_70_72_79_81_89_91_92_94_95_97_99_102_105_113_117_118_119_122_125_131_132_137_139_150_v1.csv -> llmns_id_18_28_45_48_51_57_60_70_72_79_81_89_91_92_94_95_97_99_102_105_113_117_118_119_122_125_131_132_137_139_150_v1.csv\n",
      "Renamed: llm_id_163.csv -> llmns_id_163.csv\n",
      "Renamed: llm_id_158.csv -> llmns_id_158.csv\n",
      "Renamed: llm_id_68.csv -> llmns_id_68.csv\n",
      "Renamed: llm_Active_Certifications.csv -> llmns_Active_Certifications.csv\n",
      "Renamed: llm_id_65_127_141.csv -> llmns_id_65_127_141.csv\n",
      "Renamed: llm_RptEmpRstrDetail Active.csv -> llmns_RptEmpRstrDetail Active.csv\n",
      "Renamed: llm_id_54_58_74_104_107_128_v1.csv -> llmns_id_54_58_74_104_107_128_v1.csv\n",
      "Renamed: llm_id_85.csv -> llmns_id_85.csv\n",
      "Renamed: llm_Investigations_Redacted.csv -> llmns_Investigations_Redacted.csv\n",
      "Renamed: llm_id_90.csv -> llmns_id_90.csv\n",
      "Renamed: llm_id_54_58_74_104_107_128_v2.csv -> llmns_id_54_58_74_104_107_128_v2.csv\n",
      "Renamed: llm_Invisible Institue Report.csv -> llmns_Invisible Institue Report.csv\n",
      "Renamed: llm_id_54_58_74_104_107_128_v3.csv -> llmns_id_54_58_74_104_107_128_v3.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def rename_files_in_folder(folder_path):\n",
    "    # Convert to Path object\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for file in folder.iterdir():\n",
    "        if file.is_file():  # Ensure it's a file and not a folder\n",
    "            new_name = file.name.replace('llm_','llmns_')\n",
    "            new_path = folder / new_name\n",
    "            file.rename(new_path)\n",
    "            print(f\"Renamed: {file.name} -> {new_name}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/Users/yiminglin/Documents/Codebase/Dataset/pdf_reverse/baseline_results/gpt4-vision-cleaned_predicted_kv_pairs_no_structure/'\n",
    "rename_files_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def normalize_string(s):\n",
    "    # Replace literal '\\\\n' (backslash followed by 'n') and spaces, then convert to lowercase\n",
    "    return s.replace('\\\\n', '').replace(' ', '').lower()\n",
    "\n",
    "def approx_equal(str1, str2, esp = 0.95):\n",
    "    # Calculate the Levenshtein distance (edit distance) between two strings\n",
    "    distance = Levenshtein.distance(normalize_string(str1), normalize_string(str2))\n",
    "    ratio = 1 - distance/(max(len(string1),len(string2)))\n",
    "    if(ratio > esp):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Example usage\n",
    "string1 = '62680868 I POL - Issue Doctor Patient Unity DET-DDSH'\n",
    "string2 = '62680868_POL - Issue Doctor Patient\\\\nUnity DET-DDSH'\n",
    "\n",
    "\n",
    "print(get_distance_ratio(string1,string2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0484516' can be converted to an integer.\n"
     ]
    }
   ],
   "source": [
    "string = '0484516'\n",
    "\n",
    "if string.isdigit():\n",
    "    print(f\"'{string}' can be converted to an integer.\")\n",
    "else:\n",
    "    print(f\"'{string}' cannot be converted to an integer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m string1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1234\u001b[39m  \u001b[38;5;66;03m# Contains the literal characters '\\n'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m string2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal amount\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mare_strings_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalize_string(string1),normalize_string(string2))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mare_strings_equal\u001b[0;34m(str1, str2)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mare_strings_equal\u001b[39m(str1, str2):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Normalize both strings and compare\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m normalize_string(str2)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mnormalize_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_string\u001b[39m(s):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Replace literal '\\\\n' (backslash followed by 'n') and spaces, then convert to lowercase\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "def normalize_string(s):\n",
    "    # Replace literal '\\\\n' (backslash followed by 'n') and spaces, then convert to lowercase\n",
    "    return s.replace('\\\\n', '').replace(' ', '').lower()\n",
    "\n",
    "def are_strings_equal(str1, str2):\n",
    "    # Normalize both strings and compare\n",
    "    return normalize_string(str1) == normalize_string(str2)\n",
    "\n",
    "# Example usage\n",
    "string1 = 1234  # Contains the literal characters '\\n'\n",
    "string2 = 'total amount'\n",
    "\n",
    "\n",
    "result = are_strings_equal(string1, string2)\n",
    "print(normalize_string(string1),normalize_string(string2))\n",
    "print(result)  # Output: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n"
     ]
    }
   ],
   "source": [
    "string_with_zero = '03123'\n",
    "number = int(string_with_zero)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_subsequence(seq1, seq2):\n",
    "    iter_seq2 = iter(seq2)\n",
    "    return all(item in iter_seq2 for item in seq1)\n",
    "\n",
    "seq1 = [1, 10, 22]\n",
    "seq2 = [1, 3, 6, 10, 15, 20, 22]\n",
    "\n",
    "print(is_subsequence(seq2, seq1))  # Output: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perfect_match(v1,v2):\n",
    "    if(len(v1)!=len(v2)):\n",
    "        return 0\n",
    "    delta = abs(v1[0] - v2[0])\n",
    "    for i in range(len(v1)):\n",
    "        if(abs(v1[i]-v2[i]) != delta):\n",
    "            return 0\n",
    "    return 1\n",
    "    \n",
    "def is_subsequence(seq1, seq2):#len(seq1) < len(seq2)\n",
    "    iter_seq2 = iter(seq2)\n",
    "    return all(item in iter_seq2 for item in seq1)\n",
    "\n",
    "def partial_perfect_match(v1,v2):#len(v1) < len(v2)\n",
    "    delta = abs(v1[0] - v2[0])\n",
    "    new_v1 = []\n",
    "    if(v1[0] < v2[0]):\n",
    "        for v in v1:\n",
    "            new_v1.append(v + delta)\n",
    "    else:\n",
    "        for v in v1:\n",
    "            new_v1.append(v - delta)\n",
    "    if(is_subsequence(new_v1,v2)):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s1 = [20, 240, 464, 687, 915, 1151, 1393]\n",
    "s2 = [19, 239, 463, 686, 914, 1150, 1392]\n",
    "\n",
    "print(perfect_match(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s1 = [19, 216, 358, 405, 528, 633]\n",
    "s2 = [18, 24, 215, 221, 357, 363, 404, 410, 527, 533, 632, 638]\n",
    "\n",
    "print(partial_perfect_match(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Mover's Distance: 1.875\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def calculate_emd(list1, list2):\n",
    "    # Calculate the Earth Mover's Distance\n",
    "    emd_value = wasserstein_distance(list1, list2)\n",
    "    return emd_value\n",
    "\n",
    "# Example lists\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [1,2, 3, 4, 5,7,8,9]\n",
    "\n",
    "# Calculate EMD\n",
    "similarity_number = calculate_emd(list1, list2)\n",
    "print(\"Earth Mover's Distance:\", similarity_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd_samples\n",
    "\n",
    "def EMD(PK, FK):\n",
    "    PK = [x for x in PK if x==x]#remove nan value\n",
    "    FK = [x for x in FK if x==x]\n",
    "    PK_len = len(PK)\n",
    "    FK_len = len(FK)\n",
    "    #FK_unique = list(set(FK))\n",
    "    val = PK + FK\n",
    "    try:\n",
    "        val.sort()\n",
    "    except Exception as e:\n",
    "        a = 0\n",
    "    rank = {}\n",
    "    pre_val = val[0]\n",
    "    same_st = 0\n",
    "    same_end = 0\n",
    "    #compute ranks\n",
    "    for i in range(1,len(val)):\n",
    "        v = val[i]\n",
    "        if(v == pre_val):\n",
    "            #update end index for same values \n",
    "            same_end = i\n",
    "        else:\n",
    "            #assign rank for values in [same_st, same_et]\n",
    "            mean_rank = float(same_st+same_end)/2.0\n",
    "            rank[pre_val] = mean_rank\n",
    "            #reset st, et for next value \n",
    "            same_st = i\n",
    "            same_end = i\n",
    "        #handle last iteration\n",
    "        if(i == len(val)-1):\n",
    "            if(v == pre_val):\n",
    "                mean_rank = float(same_st+same_end)/2.0\n",
    "                rank[pre_val] = mean_rank\n",
    "            else:\n",
    "                mean_rank = float(same_st+same_end)/2.0\n",
    "                rank[v] = mean_rank\n",
    "        pre_val = v\n",
    "    #assign ranks and normalization\n",
    "    PK_rank = []\n",
    "    FK_rank = []\n",
    "    scale = PK_len + FK_len\n",
    "    #Pk, FK should be divided by same scale to make sure same order will have same quantile\n",
    "    for pk in PK:\n",
    "        PK_rank.append(rank[pk]/float(scale))\n",
    "    for fk in FK:\n",
    "        FK_rank.append(rank[fk]/float(scale))\n",
    "    #get EMD\n",
    "    return emd_samples(PK_rank, FK_rank, bins = 200, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.165277626388875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = [1,10,20]\n",
    "s2 = [2,5,35]\n",
    "EMD(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(seq1, seq2, k):#len(seq1) < len(seq2)\n",
    "    i = 0\n",
    "    j = 0 \n",
    "    matched_count = 0\n",
    "    while(True):\n",
    "        if(abs(seq1[i] - seq2[j]) <= k):\n",
    "            matched_count += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        if(i == len(seq1) or j == len(seq2)):\n",
    "            break\n",
    "    if(matched_count < len(seq1)):\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = [1,10,20]\n",
    "s2 = [1,4,6]\n",
    "is_subsequence(s1,s2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.47076209541127"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 6, 10, 13, 19, 27]\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)\n",
    "cutoff = mean + 1 * std_dev\n",
    "cutoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weakly Connected Components:\n",
      "[1, 2, 3, 4]\n",
      "[5, 6]\n",
      "[8, 7]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges\n",
    "edges = [(1, 2), (2, 3), (3, 4), (5, 6), (6, 5), (7, 8)]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Find weakly connected components\n",
    "wcc = list(nx.weakly_connected_components(G))\n",
    "\n",
    "# Print the weakly connected components\n",
    "print(\"Weakly Connected Components:\")\n",
    "for component in wcc:\n",
    "    print(list(component))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Level: 50.0%\n",
      "  Sample Size = 2: CI Width = 9.54\n",
      "  Sample Size = 5: CI Width = 6.03\n",
      "  Sample Size = 10: CI Width = 4.27\n",
      "  Sample Size = 20: CI Width = 3.02\n",
      "\n",
      "Confidence Level: 75.0%\n",
      "  Sample Size = 2: CI Width = 16.27\n",
      "  Sample Size = 5: CI Width = 10.29\n",
      "  Sample Size = 10: CI Width = 7.28\n",
      "  Sample Size = 20: CI Width = 5.14\n",
      "\n",
      "Confidence Level: 90.0%\n",
      "  Sample Size = 2: CI Width = 23.26\n",
      "  Sample Size = 5: CI Width = 14.71\n",
      "  Sample Size = 10: CI Width = 10.40\n",
      "  Sample Size = 20: CI Width = 7.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_ci_width(n, sigma, confidence_level):\n",
    "    z = norm.ppf((1 + confidence_level) / 2)  # Find the z-score for the given confidence level\n",
    "    SE = sigma / np.sqrt(n)  # Calculate the standard error\n",
    "    width = 2 * z * SE  # Calculate the width of the confidence interval\n",
    "    return width\n",
    "\n",
    "# Parameters\n",
    "sigma = 10  # Assume population standard deviation is known and constant\n",
    "confidence_levels = [0.5, 0.75, 0.90]\n",
    "sample_sizes = [2, 5, 10, 20]\n",
    "\n",
    "# Calculate and display the width of CIs for different combinations\n",
    "for cl in confidence_levels:\n",
    "    print(f\"Confidence Level: {cl*100}%\")\n",
    "    for n in sample_sizes:\n",
    "        width = calculate_ci_width(n, sigma, cl)\n",
    "        print(f\"  Sample Size = {n}: CI Width = {width:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 1s: 50.00%\n",
      "95% Confidence Interval: [-19.30%, 119.30%]\n",
      "138.5903824349678\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def proportion_confidence_interval(data, confidence_level):\n",
    "    # Convert the data into a numpy array for easier manipulation\n",
    "    data = np.array(data)\n",
    "    # Calculate the proportion of 1s\n",
    "    p_hat = np.mean(data)  # This is the sample proportion\n",
    "    # Number of observations\n",
    "    n = len(data)\n",
    "    # Standard error for the proportion\n",
    "    SE = np.sqrt(p_hat * (1 - p_hat) / n)\n",
    "    # Z-score for the specified confidence level\n",
    "    z = norm.ppf((1 + confidence_level) / 2)\n",
    "    # Confidence interval calculation\n",
    "    lower_bound = p_hat - z * SE\n",
    "    upper_bound = p_hat + z * SE\n",
    "    \n",
    "    return p_hat, lower_bound, upper_bound\n",
    "\n",
    "# Example usage:\n",
    "binary_data = [1,0]  # Example binary data\n",
    "proportion, lower, upper = proportion_confidence_interval(binary_data, 0.95)  # 95% confidence level\n",
    "\n",
    "print(f\"Proportion of 1s: {proportion*100:.2f}%\")\n",
    "print(f\"95% Confidence Interval: [{lower*100:.2f}%, {upper*100:.2f}%]\")\n",
    "print((upper-lower)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, -5.853102368216048, 6.853102368216048, 6.353102368216048)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    "\n",
    "data = [1,0]\n",
    "print(mean_confidence_interval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "k=3\n",
    "s=5\n",
    "for i in range(k+1):\n",
    "    lst.append(1)\n",
    "for i in range(s - (k + 1)):\n",
    "    lst.append(0)\n",
    "\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple, banana, cherry, date\n"
     ]
    }
   ],
   "source": [
    "# Define a list of strings\n",
    "string_list = ['apple', 'banana', 'cherry', 'date']\n",
    "\n",
    "# Specify the delimiter as a comma followed by a space\n",
    "delimiter = ', '\n",
    "\n",
    "# Use join to merge the list into one string with each element separated by a comma\n",
    "merged_string = delimiter.join(string_list)\n",
    "\n",
    "print(merged_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 5, 1), (7, 8, 2), (1, 2, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Sample list of tuples\n",
    "list_of_tuples = [(1, 2, 3), (4, 5, 1), (7, 8, 2)]\n",
    "\n",
    "# Sort the list based on the third item in each tuple\n",
    "sorted_list = sorted(list_of_tuples, key=lambda x: x[2])\n",
    "\n",
    "print(sorted_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2 1\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "\n",
    "#(left, right and weight)\n",
    "#left and right starting from 0\n",
    "edges = [\n",
    "    (0, 0, 3),\n",
    "    (1, 0, 1),\n",
    "    (0, 1, 10),\n",
    "    (2, 0, 1),\n",
    "    (2, 1, 2)\n",
    "]\n",
    "\n",
    "num_nodes_A = 3\n",
    "num_nodes_B = 2\n",
    "\n",
    "adjacency_matrix = np.full((num_nodes_A, num_nodes_B), np.inf)\n",
    "\n",
    "for u, v, w in edges:\n",
    "    adjacency_matrix[u, v] = w\n",
    "\n",
    "csr_adjacency_matrix = csr_matrix(adjacency_matrix)\n",
    "\n",
    "row_ind, col_ind = min_weight_full_bipartite_matching(csr_adjacency_matrix)\n",
    "\n",
    "weight = 0\n",
    "matching = {}\n",
    "\n",
    "# Iterate over the indices returned by the matching function\n",
    "for row, col in zip(row_ind, col_ind):\n",
    "    matching[row] = col\n",
    "    weight += adjacency_matrix[row,col]\n",
    "    print(row, col)\n",
    "\n",
    "print(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArithmeticErrorreport', 'criteria:', 'incidents', 'between:', '1/1/2021', 'and', '12/31/2021', 'officer', 'detail', 'reports', '#', 'a-12', 'l.e.a.', 'data', 'technologies', 'administrative', 'database', '3/14/2022', '8:27:19', 'am', 'page', '1', 'of', '27']\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"ArithmeticErrorreport criteria: incidents between: 1/1/2021 and 12/31/2021\n",
    "officer detail reports # a-12\n",
    "\n",
    "l.e.a. data technologies                     administrative database                   3/14/2022 8:27:19 am\n",
    "\n",
    "page 1 of 27\"\"\"\n",
    "\n",
    "words = s.split()\n",
    "\n",
    "# Display the list of words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def is_metadata(meta, val):\n",
    "    if(val not in meta):\n",
    "        return 0\n",
    "    # Initialize the list to store the indices\n",
    "    indices = []\n",
    "    start = 0\n",
    "\n",
    "    # Loop to find all occurrences of the substring\n",
    "    while True:\n",
    "        index = meta.find(val, start)\n",
    "        if index == -1:\n",
    "            break\n",
    "        indices.append(index)\n",
    "        start = index + 1\n",
    "\n",
    "    for i in indices:\n",
    "        l = i-1\n",
    "        r = i+len(val)\n",
    "        f1 = 0\n",
    "        f2 = 0\n",
    "        if(l>=0 and (meta[l] == ' ' or meta[l] == '\\n' or meta[l] == ':')):\n",
    "            f1 = 1\n",
    "        if(r<len(meta) and (meta[r] == ' ' or meta[r] == '\\n' or meta[r] == ':')):\n",
    "            f2 = 1\n",
    "        if(l<0):\n",
    "            f1 = 1\n",
    "        if(r == len(meta)):\n",
    "            f2 = 1\n",
    "        if(f1 == 1 and f2 == 1):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "meta =  \"\"\"header:\n",
    "\n",
    "report criteria: incidents between: 1/1/2021 and 12/31/2021\n",
    "officer detail reports #a-12\n",
    "\n",
    "footer:\n",
    "\n",
    "l.e.a. data technologies administrative database 3/14/2022 8:27:19 am page 1 of 27\"\"\"\n",
    "\n",
    "val = 'incidents between'\n",
    "print(is_metadata(meta, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
